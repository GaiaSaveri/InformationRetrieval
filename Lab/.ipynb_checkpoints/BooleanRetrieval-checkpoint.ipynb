{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple Boolean retrieval system\n",
    "\n",
    "Except for reading the corpus, all the steps are independent from the specific corpus.\n",
    "\n",
    "With this system the retrieval is fast, while the indexing is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import total_ordering, reduce\n",
    "import csv #reading data\n",
    "import re #performing normalization --> regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posting\n",
    "\n",
    "Recall that a posting is an element of the *posting list* (a DocID associated to a term)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with total ordering we can have the total order \n",
    "#over objects of our class by defining eq and one between \n",
    "#> or <\n",
    "@total_ordering\n",
    "class Posting:\n",
    "    \n",
    "    def __init__(self, docID):\n",
    "        self._docID = docID\n",
    "        \n",
    "    def get_from_corpus(self, corpus):\n",
    "        '''\n",
    "        returns the movie description\n",
    "        '''\n",
    "        return corpus[self._docID]\n",
    "    \n",
    "    #need to define an order for the postings   \n",
    "    def __eq__(self, other):\n",
    "        return self._docID == other._docID\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self._docID > other._docID\n",
    "    \n",
    "    #__repr__ returns the object representation\n",
    "    #it can be any valid python expression such as\n",
    "    #tuple, dictionary, string etc\n",
    "    def __repr__(self):\n",
    "        #str() convert the specified value into a string\n",
    "        return str(self._docID)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posting Lists\n",
    "\n",
    "The posting list is the list of DocID associated to a term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingList:\n",
    "    \n",
    "    '''\n",
    "    class for the management of the posting list\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        attribute is a list of postings\n",
    "        '''\n",
    "        self._postings = []\n",
    "    \n",
    "    #the classmethod decorator can be applied to any\n",
    "    #method of a class\n",
    "    #this decorator will allow to call the method using\n",
    "    #class name instead of object\n",
    "    \n",
    "    #we can create a posting list from the document ID\n",
    "    #it is another constructor\n",
    "    @classmethod #called as PostingList.from_docID\n",
    "    def from_dicID(cls, docID):\n",
    "        plist = cls()\n",
    "        plist._postings = [(Posting(docID))]\n",
    "        return plist\n",
    "    \n",
    "    #we can create a posting list merging multiple posting lists\n",
    "    @classmethod\n",
    "    def from_posting_list(cls, postingList):\n",
    "        plist = cls()\n",
    "        plist._postings = postingList\n",
    "        return plist\n",
    "    \n",
    "    def merge(self, other):\n",
    "        #we have to assume that all the docID of the second list are higher\n",
    "        i = 0\n",
    "        last = self._postings[-1]\n",
    "        #skip all the elements in the second posting list that are equal to the last docID\n",
    "        #we can have the same docID multiple times\n",
    "        #when we merge lists we don't want duplicates docID\n",
    "        while(i < len(other._postings) and last == other._postings[i]):\n",
    "            i += 1\n",
    "        #true iff all DocID of other are higher\n",
    "        self._postings += other._postings[i:]\n",
    "        \n",
    "    def intersection(self, other):\n",
    "        intersection = []\n",
    "        i = 0 #index for the first list\n",
    "        j = 0 #index for the second posting list\n",
    "        \n",
    "        #until we reach the end of one of the posting lists\n",
    "        while(i < len(self._postings) and j < len(other._postings)):\n",
    "            #if postings are equal, we add to the intersection\n",
    "            if(self._postings[i] == other._postings[j]):\n",
    "                intersection.append(self._postings[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            #we have to switch to the right in the first\n",
    "            elif(self._postings[i] < other._postings[j]):\n",
    "                i += 1\n",
    "            #we have to switch to the right in the second\n",
    "            else:\n",
    "                j += 1\n",
    "        return PostingList.from_posting_list(intersection)\n",
    "        \n",
    "    def union(self, other):\n",
    "        union = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while(i < len(self._postings) and j < len(other._postings)):\n",
    "            #wheter DocIds are in one, the other or both posting lists\n",
    "            #we need to add them to the union\n",
    "            #and switch the counter accordingly\n",
    "            if(self._postings[i] == other._postings[j]):\n",
    "                union.append(self._postings[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif(self._postings[i] < other._postings[j]):\n",
    "                #we append the smallest one\n",
    "                union.append(self._postings[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                union.append(other._postings[j])\n",
    "                j += 1\n",
    "        #we can have a list that is not emptied\n",
    "        for k in range(i, len(self._postings)):\n",
    "            union.append(self._postings[k])\n",
    "        for k in range(j, len(other._postings)):\n",
    "            union.append(other._postings[k])\n",
    "        return PostingList.from_posting_list(union)\n",
    "    \n",
    "    #the map function applies a given function to each item\n",
    "    #of an iterable and return a list of the results\n",
    "    \n",
    "    #we want the collection of all documents of the posting lists\n",
    "    def get_from_corpus(self, corpus):\n",
    "        #using get_from_corpus from Posting class\n",
    "        return list(map(lambda x: x.get_from_corpus(corpus), self._postings))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \", \".join(map(str, self._postings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: 3, 6\n"
     ]
    }
   ],
   "source": [
    "class ImpossibleMergeError(Exception):\n",
    "    pass\n",
    "\n",
    "@total_ordering \n",
    "class Term:\n",
    "    \n",
    "    def __init__(self, term, docID):\n",
    "        self.term = term\n",
    "        #recall that PostingList from_docId \n",
    "        #creates a posting lists with only that DocID inside\n",
    "        self.posting_list = PostingList.from_dicID(docID)\n",
    "    \n",
    "    def merge(self, other):\n",
    "        if(self.term == other.term):\n",
    "            #using merge from PostingList class\n",
    "            self.posting_list.merge(other.posting_list)\n",
    "        else:\n",
    "            raise ImpossibleMergeError #some kind of error\n",
    "            \n",
    "    #we need to order the terms\n",
    "    def __eq__(self, other):\n",
    "        return self.term == other.term\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self.term > other.term\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.term + \": \" + repr(self.posting_list)\n",
    "\n",
    "#example\n",
    "x = Term(\"cat\", 3)\n",
    "y = Term(\"cat\", 6)\n",
    "x.merge(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eg this is a text'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(text):\n",
    "    '''\n",
    "    remove punctuation and everything that is not a word\n",
    "    '''\n",
    "    #way of matching the text and substitute it\n",
    "    # \\W means not sth alphanumeric\n",
    "    # \\s not a space\n",
    "    # - not a dash\n",
    "    no_punctuation = re.sub(r'[^\\w^\\s^-]','',text)\n",
    "    downcase = no_punctuation.lower()\n",
    "    return downcase\n",
    "\n",
    "def tokenize(movie):\n",
    "    '''\n",
    "    normalize movie description\n",
    "    '''\n",
    "    text = normalize(movie.description)\n",
    "    return list(text.split())\n",
    "\n",
    "class InvertedIndex:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._dictionary = []\n",
    "    \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        #keys are the tokens\n",
    "        #actual terms are the values\n",
    "        intermediate_dict = {}\n",
    "        for docID, document in enumerate(corpus):\n",
    "            tokens = tokenize(document)\n",
    "            for token in tokens:\n",
    "                term = Term(token, docID)\n",
    "                try:\n",
    "                    #merge the posting lists\n",
    "                    intermediate_dict[token].merge(term)\n",
    "                except KeyError:\n",
    "                    #insert the new id\n",
    "                    intermediate_dict[token] = term\n",
    "            if(docID % 1000 == 0):\n",
    "                print(\"ID: \" + str(docID))\n",
    "        #now we have all the terms and we can create the actual inverted index\n",
    "        idx = cls()\n",
    "        #the dictionary is the sorted list of all the terms\n",
    "        idx._dictionary = sorted(intermediate_dict.values())\n",
    "        return idx\n",
    "    \n",
    "    #we have to index the inverted index using terms\n",
    "    def __getitem__(self, key):\n",
    "        #since everything is sorted in principle we could do a binary search\n",
    "        for term in self._dictionary:\n",
    "            if term.term == key: #the one we are searching for\n",
    "                return term.posting_list\n",
    "        raise KeyError\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"A dictionary with \" + str(len(self._dictionary)) + \" terms\"\n",
    "    \n",
    "\n",
    "#example\n",
    "normalize(\"e.g., this is a text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the corpus\n",
    "\n",
    "Dependent on the specific corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDescription:\n",
    "    '''\n",
    "    container for all the info we have about the movie\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, title, description):\n",
    "        self.title = title\n",
    "        self.description = description\n",
    "        \n",
    "    def __repr__(self): \n",
    "        return self.title\n",
    "    \n",
    "    \n",
    "def read_movie_description():\n",
    "    #hardcode the names of the dataset's files\n",
    "    filename = 'plot_summaries.txt'\n",
    "    movie_names_file = 'movie.metadata.tsv'\n",
    "    with open(movie_names_file, 'r') as csv_file:\n",
    "        movie_names = csv.reader(csv_file, delimiter='\\t')\n",
    "        #dictionary where the index will be the ID of the movie\n",
    "        #and the value the title of the movie\n",
    "        names_table = {}\n",
    "        for name in movie_names:\n",
    "            names_table[name[0]] = name[2]\n",
    "    #open the file containing the descriptions\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        descriptions = csv.reader(csv_file, delimiter='\\t')\n",
    "        #corpus is a list of objects\n",
    "        corpus = []\n",
    "        #we wrap into a try block since there are some descriptions\n",
    "        #with errors in the ID\n",
    "        for desc in descriptions:\n",
    "            try:\n",
    "                movie = MovieDescription(names_table[desc[0]], desc[1])\n",
    "                corpus.append(movie)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit Distance\n",
    "\n",
    "We want to add a few methods to answer query correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_distance(u, v):\n",
    "    #number of rows is the length of the first word + 1\n",
    "    nrows = len(u) + 1\n",
    "    #number of columns is the length of the second word + 1 \n",
    "    ncols = len(v) + 1\n",
    "    M = [[0] * ncols for  i in range(0, nrows)]\n",
    "    for i in range(0, nrows):\n",
    "        M[i][0] = i\n",
    "    for j in range(0, ncols):\n",
    "        M[0][j] = j\n",
    "    for i in range(1, nrows): \n",
    "        for j in range(1, ncols):\n",
    "            candidates = [M[i-1][j] + 1, M[i][j-1] + 1]\n",
    "            if (u[i-1] == v[j-1]):\n",
    "                candidates.append(M[i-1][j-1])\n",
    "            else:\n",
    "                candidates.append(M[i-1][j-1] + 1)\n",
    "            M[i][j] = min(candidates)\n",
    "    return M[-1][-1]\n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "#the first letter is usually not mispelled\n",
    "def find_nearest(word, dictionary, keep_first=False):\n",
    "    if keep_first:\n",
    "        #all the words in the dictionary where the first letter is equal to the\n",
    "        #first letter of the word\n",
    "        dictionary = [w for w in dictionary if w[0]==word[0]]\n",
    "    distances = map(lambda x: edit_distance(word, x), dictionary)\n",
    "    return min(zip(distances, dictionary))[1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRsystem:\n",
    "    #in the system we have both the index and the corpus\n",
    "    def __init__(self, corpus, index):\n",
    "        self._corpus = corpus\n",
    "        self._index = index\n",
    "    \n",
    "    #we want to geenrate the entire inverted index \n",
    "    #calling the constructor\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        index = InvertedIndex.from_corpus(corpus)\n",
    "        return cls(corpus, index)\n",
    "    \n",
    "    #conjuction query?\n",
    "    def answer_query(self, words): \n",
    "        #same normalization in the query and in the index\n",
    "        norm_words = map(normalize, words)\n",
    "        postings = map(lambda w: self._index[w], norm_words)\n",
    "        plist = reduce(lambda x, y: x.intersection(y), postings)\n",
    "        return plist.get_from_corpus(self._corpus)\n",
    "    \n",
    "    def answer_query_sc(self, words):\n",
    "        norm_words = map(normalize, words)\n",
    "        postings = []\n",
    "        for w in norm_words:\n",
    "            try: \n",
    "                res = self._index[w]\n",
    "            except KeyError:\n",
    "                dictionary = [t.term for t in self._index._dictionary]\n",
    "                sub = find_nearest(w, dictionary, keep_first=True)\n",
    "                print(\"{} not found. Did you mean {}?\".format(w, sub))\n",
    "                res = self._index[sub]\n",
    "            postings.append(res)\n",
    "        plist = reduce(lambda x, y: x.intersection(y), postings)\n",
    "        return plist.get_from_corpus(self._corpus)\n",
    "    \n",
    "\n",
    "#permorm a query on a IR system\n",
    "def query(ir, text):\n",
    "    words = text.split()\n",
    "    answer = ir.answer_query(words)\n",
    "    for movie in answer:\n",
    "        print(movie)   \n",
    "        \n",
    "def query_sc(ir, text):\n",
    "    words = text.split()\n",
    "    answer = ir.answer_query_sc(words)\n",
    "    for movie in answer:\n",
    "        print(movie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus = read_movie_description()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ir = IRsystem.from_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query(ir, \"batman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0\n",
      "ID: 1000\n",
      "ID: 2000\n",
      "ID: 3000\n",
      "ID: 4000\n",
      "ID: 5000\n",
      "ID: 6000\n",
      "ID: 7000\n",
      "ID: 8000\n",
      "ID: 9000\n",
      "ID: 10000\n",
      "ID: 11000\n",
      "ID: 12000\n",
      "ID: 13000\n",
      "ID: 14000\n",
      "ID: 15000\n",
      "ID: 16000\n",
      "ID: 17000\n",
      "ID: 18000\n",
      "ID: 19000\n",
      "ID: 20000\n",
      "ID: 21000\n",
      "ID: 22000\n",
      "ID: 23000\n",
      "ID: 24000\n",
      "ID: 25000\n",
      "ID: 26000\n",
      "ID: 27000\n",
      "ID: 28000\n",
      "ID: 29000\n",
      "ID: 30000\n",
      "ID: 31000\n",
      "ID: 32000\n",
      "ID: 33000\n",
      "ID: 34000\n",
      "ID: 35000\n",
      "ID: 36000\n",
      "ID: 37000\n",
      "ID: 38000\n",
      "ID: 39000\n",
      "ID: 40000\n",
      "ID: 41000\n",
      "ID: 42000\n"
     ]
    }
   ],
   "source": [
    "corpus = read_movie_description()\n",
    "ir = IRsystem.from_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yioda not found. Did you mean yoda?\n",
      "lukke not found. Did you mean luke?\n",
      "darhth not found. Did you mean darth?\n",
      "Star Wars Episode V: The Empire Strikes Back\n",
      "Something, Something, Something Dark Side\n",
      "Return of the Ewok\n",
      "Star Wars Episode III: Revenge of the Sith\n",
      "Star Wars Episode VI: Return of the Jedi\n",
      "It's a Trap!\n"
     ]
    }
   ],
   "source": [
    "query_sc(ir, \"yioda lukke darhth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
