# Web structure

The Web relies on a simple, open client-server design: the server communicates with the client via a protocol (e.g. *http* or hypertext protocol) that is lightweight and simple, encoded in a simple markup language called *HTML* (HypeText Markup Language), the client (generally a browser) can ignore what it does not understand.

The basic operation is: a client sends a *http request* to a *web server*. The browser specifies a *URL* (protocol + domain + path).

Some terminology:  

* **HTTP** and **HTTPS** are protocols to transmit pages.

* **HTML** is the markup language used to encode web pages.

* **URL** (Universal Resource Locator).

* **Static web pages** the content does not change at each request, opposite to **Dynamic web pages** automatically generated pages (e.g in response to a query to a database).

Inside the webpage the links are usually represented with `<a href=URL> Anchor text </a>` (in the origin of the hyperlink of a page), the **anchor text** is the text visualised for a link.

We can view the (static) web consisting of HTML pages together with the hyperlinks between them as a directed graph in which each web page is a node and each hyperlink a directed edge, it is called the *web graph*. This directed graph is not strongly connected.

* **In-links** is the set of links that refer to a web page (we are inside a directed graph), **Out-links** is the set of links from a web page (this can be obtained by looking at the web page).

The web can be seen as a graph (at different levels), indeed:

* page is a node and edges are given by the links;

* at higher level, a PLD (pay-level-domain) is a node with outgoing edges given by all the links contained in the pages on the PLD (we are looking inside website, not single pages).

The distribution of in-degrees and out-degrees of the nodes is far from the random graph model, more closely modelled by a power law distribution $f(x)=ax^{-k}$ (this reminds the Zipf's law).

The structure of web links is bowtie shape, with three major categories of web pages that are referred to as IN, OUT and SCC. We have SCC, where following hyperlinks it is possible to reach each other page in SCC. We have IN, which are pages that can reach SCC, but cannot be reached by pages in SCC. Then we have OUT, that are pages that can be reached from SCC, but cannot reach SCC. The remaining pages fall into : tubes, that are direct links from IN to OUT, and tendrils, that are pages reachable from IN that lead nowhere or that reach only pages in OUT.

## Deep web

The **deep/hidden web** is the part of the web is the set of pages that are difficult or impossible to index (don't confuse with the dark web, which is purposefully made inaccessible). It is estimated to be larger than the conventional web: it contains private sites (login may be needed, not incoming links), form results, and scripted pages (where the links are generated by scripts).

# Web Crawlers


**Web crawling** is the process of gathering pages from the Web to index them (and to support a search engine). The objective of a crawling is to quickly and efficiently gather as many useful web pages as possible, together with the link structure that interconnects them. This process is carried on by *web crawlers* also called *spiders*.

Web crawling has to deal with the scale of the web and the fact that the content to index is not under control of the people building the index.

We have:

* **unseen web**: set of unvisited web pages for which the URL is not known;

* **URL frontier**: set of pages with URL found by the crawler but not visited;

* **set of visited pages**: set of pages that the crawler has visited;

* **seed pages**: set of known web pages from which the crawling starts.  

## Essential properties of a Web Crawler

Features that every web crawler must have are:

* **Robustness**: a web crawler must not be blocked by *spider traps*, web pages built to force a crawler to fetch an infinite amount of pages from a specific domain (note that not all spider traps are malicious). Crawlers must be designed to be resilient to such traps.

* **Politeness**: a web crawler cannot overload a web server with requests (web servers have both implicit and explicit policies regulating the rate a crawler can visit them). All requests to a domain must be adequately spaced in time and policies like the one in "robot.txt" must be adhered to.

A `robot.txt` (*robot exclusion protocol*) file in a web server provides some information on what a crawler is allowed to index. The `robot.txt` file must be fetched from a website to test whether the URL under consideration passes the robot restrictions, and can therefore be added to the URL frontier.

## Good properties of a web crawler

A web crawler should possibly be:

* **distributed**: indexing the entire web from a single machine is infeasible, the web crawler should be able to execute from multiple machines.

* **scalable**: it should be possible to increase the crawl rate by simply adding more machine and bandwidth.

* **performance and efficiency**: the crawler should try to make efficient use of system resources (e.g. by not blocking when waiting for the response from a server).

* **quality**: the crawler should have a bias towards "useful" pages (there is a significant amount of web pages that are of poor utility).

* **freshness**: the content on the web is always changing, thus the crawler should revisit already visited pages to obtain a fresh copy (a crawler should visit a page with frequency that approximate the rate of change of the page).

* **extensible**: there might be new data format, new protocols etc and the crawler should be able to extend to handle them. This demands the crawler architecture to be modular.

## Architecture of a web crawler

The basic structure of a web crawler is made of several modules:

* fetch module: retrieve a URL (using the http protocol) to crawl from the URL frontier;

* DNS (Domain Name Service) solver: find which IP address corresponds to `www.example.com`, it determines the web server from which to fetch the page specified by a URL;

* fetch module: ask for the web page to the server, then receives the web page;

* parse: the page is parsed, the links and the main content are extracted;

* remove duplicate pages (duplicate elimination module): before indexing the page is checked with a set of *fingerprints* of other pages to verify if it is a duplicate;

* URL filter: the newly extracted URL are normalised and filtered to eliminate the ones that should not be crawled;

* remove duplicate URL: before being inserted into the URL frontier (i.e. the set of URL to visit), already visited URL are removed.  

Summarising, the basic operation of any hypertext crawler is that the crawler begins with one or more URLs that constitute a *seed set*. It picks a URL from this seed set, then fetches the web page at that URL. The fetched page is then parsed, to extract both the text and the links from the page. The extracted text is fed to text indexer, the extracted links are then added to a *URL frontier*, which at all times consists of URLs whose corresponding pages have yet to be fetched by the crawler. Initially the URL frontiers contains the seed set, as pages are fetched, the corresponding URLs are deleted from the IRL frontier.

## Selection of the next URL

In principle we want to keep only one connection open to the host, ensure that a waiting time of at least a few seconds between requests, have a bias for pages with higher priority.

Multiple threads can extract from the same URL frontier at the same time.

The goals of a polite and prioritising URL frontier are:

* only one connection is open at a time to any host;

* a waiting time of a few seconds occurs between successive requests to a host;

* high-priority pages are crawled preferentially.

The two major submodules of a URL frontier are a set of $F$ front queues and a set of $B$ back queues. All these are FIFO queues. The front queues implement the prioritisation and the back queues implement politeness.

Each of the $B$ back queues maintains the following invariants: it is nonempty while the crawl is in progress, and it only contains URLs from a single host. Whenever a back queue is empty, it is refilled from a front-queue.

In addition, we maintain a heap with one entry for each back queue, the entry being the earliest time $t_e$ at which the host corresponding to that queue can be contacted again.

A crawler thread requesting a URL from the frontier extracts the root of the heap and if necessary waits until the corresponding time entry $t_e$. It then takes the URL at the head of the back queue corresponding to the extracted heap root, and proceeds to fetch the URL. After fetching, it checks if the back queue is empty. If so, it picks a front queue and extract from its head.We examine the new URL to check whether there is already a back queue holding URLs from its host, if so, it is added to that queue and we reach back to the front queues to find another candidate URL. The choice of front queue is biased towards queues of higher priority, ensuring that URLs of high priority flow more quickly into the back queues.

In the **URL frontier** we have a *prioritiser* that manages the priorities (assigns an integer priority between $1$ and $F$ to each new URL), and we have $F$ front FIFO queues (one for each priority). We don't extract directly from these queues, there is a biased front queue selector and there are $B$ back queues to manage politeness. Each of the back queues has the property that: the queue is non-empty while the crawling is in progress, each queue contains URL from a single host. To do so we need to keep mapping from hosts to queues (there is a back queue router).

We keep also a heap that returns the minimum time to wait to contact again the host. We extract the top of the heap, wait the required time and extract a new URL from the corresponding queue.

If the queue is now empty, then a new URL is taken from the front queues in a biased manner (i.e. higher probability of being selected to higher priority queues).

If the URL is from an host with an already assigned queue, then it is inserted in that queue, and the extraction is repeated.

This works because first we fix priority, but then on the second layer of the architecture we have waiting time, so that we avoid overloading every single host.

## Freshness

A HEAD request is a kind of request where the server sends some information about a page, but not the page itself. Among the information there is the *last-modified* time.

We perform a HEAD request that give us the last modified time and we use it to check pages for freshness. However, it is impossible to constantly check all pages: we must decide a policy on what pages to check. For this aim we have two metrics: **freshness** and **age**.

**Freshness** is a binary concept, a page is fresh if the crawler has the most recent copy of the page, otherwise the page is stale. The freshness is the fraction of web pages that are currently fresh.

If we optimise only for freshness, there are same unintended consequences: suppose that a page updates very frequently, then we will almost always have a stale copy of the page.

If we have limited resources for crawling, then a good strategy would be to never crawl that page again: it will be always stale after a very short time. But this is not what the user wants, so we can optimise for **age**.

## Age

**Age** is the time that has passed between the update and the time were we crawled the webpage, so ages always increases as more time passes between update and crawl. We want to minimise the age. It is a more refined way of finding outdated pages: a page starts ageing when it is modified. Its age returns to 0 when it is crawled again.

Suppose that a page is updated $\lambda$ times a day: the its expected age at time $t$ after it was visited last time is $$Age(\lambda,t)=\int_{0}^{t} P(Change\ at\ time x)(t-x)dx$$.

The probability of a page changing at a certain time $x$ can be estimated: according to studies, the updates to a web page follows a Poisson distribution, hence we obtain: $$Age(\lambda, t) = \int_{0}^{t} \lambda e^{-\lambda x} (t-x) dx$$

We should want to minimise the expected age of a set of pages we will visit them all (with freshness we decided not to visit frequently changing web pages).

$$Age(\lambda,t) = \frac{t+\lambda e^{-\lambda t}-1}{\lambda}$$

$$\frac{\partial^2 Age(\lambda, t)}{\partial^2 t^2}=\lambda e^{-\lambda t}$$

The rate of increase of the age function (i.e. the second derivative) is always positive for $\lambda >0$ (which is always the case). So not visiting a web page has an increasing cost the older the page gets. Hence we will never conclude that we do not have to visit a web page.

We want to visit fresh pages is equivalent to say that we want to minimise the age of a page.

## Storage

**Bigtable** is a NoSQL database used internally at Google (Apache HBase and Apache Accumulo are similar).

Idea is that we have more or less a very fancy hash table: maps three a values into a byte array: a row and a column keys as arbitrary strings plus a timestamp. It is a distributed database initially developed to store web pages.

Conceptually is one enormous table where everything is stored as triple, in practice the table is actually split into multiple **tablets** which are distributed across thousands of machines.

Each database contains exactly one (logical) table, data are saved in an immutable way (we rewrite with a new timestamp), this helps in case of failure recovery.

Any change to a tablet is recorded in a transaction log. Recent data are stored in RAM and periodically merged to reduce the total number of disk files.

Data are organised by rows. Rows are partitioned into tablets based on their row key. Each row can have a different (and large) number of columns. Columns are grouped in column groups and all rows have the same column groups.

# Duplicates and near-duplicates

There is the problem of duplicated web pages: studies show that about $30\%$ of the crawled pages are duplicates or near-duplicates of the other $70\%$. Many of these are legitimate copies, for instance certain information repositories are mirrored simply to provide redundancy and access reliability. Search engines try to avoid indexing multiple copies of the same content, to keep down storage and processing overheads.

Duplicates can be created by spam or plagiarism but also via mirror sites. They provide very little information to the user while consuming resources for crawling and indexing.  

## Duplicates

The detection of exact duplicates is relatively easy: it can be performed by comparing the **checksum** of the documents. One of the simplest kinds of checksum is to simply sum all the bytes in the document.

for each webpage a *fingerprint* that is a succinct digest of the characters on that page is created. Then, whenever the fingerprints of two web pages are equal, we test whether the pages themselves are equal. But this fails to capture *near-duplication*.

## Near duplicates

Detecting near-duplicates is more complex, but even *defining* them is more problematic (e.g. we can have same text but different advertising/formatting, or slight difference in the text due to small edits).

In general a similarity measure is defined and two documents are considered near-duplicates above a certain threshold.

Detecting near-duplicates can happen in two scenarios:

* **search**: when the goal is to find the duplicates of a given document;

* **discovery**: when, given a collection, the goal is to find all pairs of duplicates or near duplicates.

What we usually use in case of discovery is called **fingerprinting**. A possible algorithm is, given a web page:

1. All non-word content is removed. The document is parsed into words;

2. Words are grouped in $n$-grams for some $n$;

3. A subset of $n$-grams is selected;

4. The $n$-grams are hashed;

5. The hashes are stored in an inverted index.

Two documents are considered near-duplicates if they share enough $n$-grams (by measuring e.g. the Jaccard coefficient). It is essential to have a good way of selecting which subset of $n$-grams to keep: random selection is a bad choice (the overlap between randomly selected $n$-grams of identical documents can be low), we can select all $n$-grams starting with the same letter, another choice is to select all $n$-grams with hash value equal to $0mod\ p$ for some choice of $p$.

**Simhash** is a more recent fingerprint technique that consists in (given a web page):

1. Extract a set of features (e.g. word) each with a weight (e.g. frequency);

2. For each word compute a unique hash of $b$ bits (the desired size of the fingerprint);

3. Start with a vector of size $b$ with all positions initially set to $0$, look at the first bit of the hash of every word. Add the weight to the word if the bit is $1$. Subtract the weight of the word if the bit is $0$.

4. Do the same for all the others bits, as to obtain a sequence of $b$ bits by setting a bit to $1$ for positive values and to $0$ otherwise. This is the **hash value of the document**.

# Finding the content

We want to use the actual main content of the page. The main content of the page might be only a fraction of the total area, the rest is advertisement, navigation links etc. From the point of view of the user, the rest is *noise*, that can have a negative effect on the ranking. We need a way to identify the non-main content of the page and either ignore it or reduce its weight. An observation is that, usually, the main content of the page contains less tags than the rest of the page.  

We actually split the web page in tokens and we count number of tags and how the number of tags changes with the number of tokens. When we have a lot of tokens and only a few tags, then this could be the main content.

So we find two cutting points $i$ and $j$ with $1\leq i\leq j\leq N$ maximising: $$\sum_{k=1}^{i-1}b_k + \sum_{k=i}^{j}(1-b_k)+\sum_{k=j+1}^{N}b_k$$ where $b_k$ is $1$ if the $k$-th term is a tag, and $0$ otherwise, considering documents as binary vectors of length $N$ (number of tokens). I.e. we are maximising the sum of the tags before content, non-tags in the content, tags after content.

Another possibility is to use some heuristics and look ant the **DOM** (Document Object model): to parse a webpage a browser construct a representation using the HTML tags.

It is a tree-like structure that can be navigated to find the major components of a web page. A set of heuristics and filtering techniques can be used to remove images, advertising and leave only the content. It is also possible to analyse the visual feature of a page to identify the location of the main content.

# Pagerank

**PageRank** is the algorithm used by Google in ranking the pages in its results. The important part is that, instead of looking only at the content of the page, we take advantage of which pages links to which other pages, so we are looking at the connections between the pages. Pages that are linked by lots of other pages are usually more important.

The main idea is that we want to assign a static score to each webpage (essentially using links) which is independent from the query. Our model is that of a user that clicks randomly the links contained in a webpage. We want to know the limit distribution of "where the user is" across all the pages. Moreover a user is without any memory of the pages he visited. So it is a Markov Chain. We formalise this random walk on a graph by defining a stochastic matrix.

We want  to find the probability distribution of the web page our idealised user is in when the time tends to infinity. This is equivalent to finding the stationary distribution of the Markov chain.

A first problem that can appear in defining the stochastic matrix $R$ is the presence of "dangling nodes", i.e. nodes without outgoing edges. A simple fix is to assume that the user will go somewhere else uniformly at random.

Also we have pages without incoming links, these will have probability $0$ to be visited again. Then we can pages with outgoing links that form a group of nodes without outgoing edges: we can never leave them once entered.

The way in which these are solved is called **teleporting**: it is common to have sinks where it is impossible to exit by only following the links or pages that we cannot go back to (teleport operation is born to solve this).

This produces an imbalance in our scores, that can potentially be exploited. In fact our idealised user can be a little bit smarter. At every page it can: move following one of the links in the page or go to a random page.

So we have a probability $1-\alpha$ to move to a linked page, and a probability $\alpha$ to move to a random page. $\alpha>0$ can be considered a "damping factor" or "probability that our user decides to go to another website".

We assign probability $\frac{1}{N}$ of landing to any particular page (so that we have a *jumping vector*). The user invokes the teleport operation with probability $0<\alpha <1$, with $\alpha$ fixed parameter chosen in advance.

We usually write the transition matrix as $P=\alpha \bar{1}^{T} \bar{J} + (1-\alpha)R$.

With the teleporting trick we can go to any other web page in one step, which means that all entries of $P$ are positive.

Our goal is to have some score that is independent from where we started. All entries in $P$ are positive if $\alpha>0$, so we can apply a formulation of Perron-Frobenius theorem:  

If $P$ is a positive row (or column) stochastic matrix, then:

1. The eigenvalue $1$ is the largest eigenvalue and has multiplicity $1$;

2. There is a unique stochastic eigenvector for the eigenvalue $1$.

The PageRank vector of the transition matrix $P$ is the **unique** stochastic eigenvector corresponding to the eigenvalue $1$: $\bar{\pi}=\lambda \bar{\pi}$.

In our case $\lambda=1$ (stationary distribution), thus: $\bar{\pi}=\bar{\pi}$, which is a linear system. However $P$ is a very big matrix (few billions of rows probably).

We can compute it in an iterative way (instead of solving the PageRank exactly), in fact, it converges quite rapidly. The main idea is that, if we start from a stochastic vector $\bar{x}$, maybe giving equal probability to each page, then $\bar{x}P^t$ for a large enough $t$ would be a good approximation of the exact solution $\bar{\pi}$.k

## Topic specific PageRank

In addition to computing PageRank scores for all pages we can limit the computation to single topics. We can do this by simply change the probability distribution for the teleportation (i.e. for the jump vector). So the teleporting to a random page is now considered *non-uniform*.

We start with a (non-empty) set $S$ of pages specific to a certain topic, then our jumps can only be inside $S$.

## Personalised Pagerank

We might want to add a special PageRank score for every user (which can have a mixture of interests from multiple topics), depending on the topics he/she is interested in. However, performing the PageRank computation for every user is too expensive: we can use the *linearity* of PageRank. We assume that an individual's interests can be well-approximated as a linear combination of a small number of topic page distributios.

Let $S_1$ and $S_2$ be two disjoints topic specific pages, suppose that the corresponding PageRank scores are $\bar{\pi_1}$, $\bar{\pi_2}$.

For a user that is interested in the first topic with weight $w_1\geq 0$ and in the second topic with weight $w_2\geq 0$, with $w_1+w_2=1$, we can compute the corresponding PageRank score as $w_1\bar{\pi_1} + w_2\bar{\pi_2}$.

Hence we can compute personalised PageRank scores with a weighted sum of pre-computed scores.

## Manipulation of PageRank

There is an implicating conflict between the indexing and the people managing the websites: Google needs to keep the search results relevant to the user, normal and spam websites wants to rank high in the search results.

To mitigate some of the problems, the `rel=nofollow` attribute was added to HTML, pages with this attribute are not considered for the purpose of computing the PageRank score.  

# Hits

**Hits** is an algorithm that splits the score in two parts: **hubs** and **authorities**.  Hubs are pages with lot of other pages links, authorities are pages linked by a lot of other pages.

Hubs are pages important not for their content but for the links that they provide toward pages with interesting content.

Authorities are pages that are important for their content, therefore they are linked by many pages.

The approach is now to use hub pages to discover authority pages.

The **hyperlink-induced topic search (HITS)** algorithm assigns two different scores to each page, an authority and a nub score. The main idea behind the algorithm is:

* a good hub points to pages with high authority score;

* a good authority is pointed by pages with high hub scores.

Differently from PageRank, HITS is usually computed when the query is executed. A set of pages is obtained by some other methods (e.g. by looking at the text content of the page). We consider the subset of pages that we have retrieved (which will probably have very few links to each other) as a **root set**. We add to the root set all pages pointed and pointing to it.

In this extended set we compute the two scores, that can now be used for ranking.

The *hub score* $h(x)$ of a page $x$ is defined as: $$h(x)=\sum_{x\rightarrow y} a(y)$$ i.e. the sum of all authority scores for all the pages linked by $x$.

The *authority score* $a(x)$ of a page $x$ is defined as: $$a(x)=\sum_{y\rightarrow y}h(y)$$ i.e. the sum of the hub scores for all pages that links to $x$.

We can compute these scores analytically, but an iterative method is often used.
