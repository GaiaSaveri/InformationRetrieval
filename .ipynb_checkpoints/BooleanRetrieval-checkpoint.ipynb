{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple Boolean retrieval system\n",
    "\n",
    "Except for reading the corpus, all the steps are independent from the specific corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import total_ordering, reduce\n",
    "import csv #reading data\n",
    "import re #performing normalization --> regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@total_ordering\n",
    "class Posting:\n",
    "    \n",
    "    def __init__(self, docID):\n",
    "        self._docID = docID\n",
    "        \n",
    "    def get_from_corpus(self, corpus):\n",
    "        '''\n",
    "        returns the movie description\n",
    "        '''\n",
    "        return corpus[self._docID]\n",
    "    \n",
    "    #need to define an order for the postings   \n",
    "    def __eq__(self, other):\n",
    "        return self._docID == other._docID\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self._docID > other._docID\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self._docID)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posting Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingList:\n",
    "    \n",
    "    '''\n",
    "    class for the management of the posting list\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        '''\n",
    "        attribute is a list of postings\n",
    "        '''\n",
    "        self._postings = []\n",
    "    \n",
    "    #we can create a posting list from the document ID\n",
    "    #it is another constructor\n",
    "    @classmethod #called as PostingList.from_docID\n",
    "    def from_dicID(cls, docID):\n",
    "        plist = cls()\n",
    "        plist._postings = [(Posting(docID))]\n",
    "        return plist\n",
    "    \n",
    "    #we can create a posting list merging multiple posting lists\n",
    "    @classmethod\n",
    "    def from_posting_list(cls, postingList):\n",
    "        plist = cls()\n",
    "        plist._postings = postingList\n",
    "        return plist\n",
    "    \n",
    "    def merge(self, other):\n",
    "        #we have to assume that all the docID of the second list are higher\n",
    "        i = 0\n",
    "        last = self._postings[-1]\n",
    "        #skip all the elements in the second posting list that are equal to the last docID\n",
    "        #we can have the same docID multiple times\n",
    "        #when we merge lists we don't want duplicates docID\n",
    "        while(i < len(other._postings) and last == other._postings[i]):\n",
    "            i += 1\n",
    "        self._postings += other._postings[i:]\n",
    "        \n",
    "    def intersection(self, other):\n",
    "        intersection = []\n",
    "        i = 0 #index for the first list\n",
    "        j = 0 #index for the second posting list\n",
    "        \n",
    "        #until we reach the end of one of the posting lists\n",
    "        while(i < len(self._postings) and j < len(other._postings)):\n",
    "            if(self._postings[i] == other._postings[j]):\n",
    "                intersection.append(self._postings[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif(self._postings[i] < other._postings[j]):\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "            return PostingList.from_posting_list(intersection)\n",
    "        \n",
    "    def union(self, other):\n",
    "        union = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while(i < len(self._postings) and j < len(other._postings)):\n",
    "            if(self._postings[i] == other._postings[j]):\n",
    "                union.append(self._postings[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif(self._postings[i] < other._postings[j]):\n",
    "                #we append the smallest one\n",
    "                union.append(self._postings[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                union.append(other._postings[j])\n",
    "                j += 1\n",
    "        #we can have a list that is not emptied\n",
    "        for k in range(i, len(self._postings)):\n",
    "            union.append(self._postings[k])\n",
    "        for k in range(j, len(other._postings)):\n",
    "            union.append(other._postings[k])\n",
    "        return PostingList.from_posting_list(union)\n",
    "    \n",
    "    #we want the collection of all documents of the posting lists\n",
    "    def get_from_corpus(self, corpus):\n",
    "        return list(map(lambda x : x.get_from_corpus(corpus), self.postings))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \", \".join(map(str, self._postings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: 3, 6\n"
     ]
    }
   ],
   "source": [
    "class ImpossibleMergeError(Exception):\n",
    "    pass\n",
    "\n",
    "@total_ordering #all possible comparisons are already defined\n",
    "class Term:\n",
    "    \n",
    "    def __init__(self, term, docID):\n",
    "        self.term = term\n",
    "        self.posting_list = PostingList.from_dicID(docID)\n",
    "    \n",
    "    def merge(self, other):\n",
    "        if(self.term == other.term):\n",
    "            self.posting_list.merge(other.posting_list)\n",
    "        else:\n",
    "            raise ImpossibleMergeError #some kind of error\n",
    "            \n",
    "    #we need to order the terms\n",
    "    def __eq__(self, other):\n",
    "        return self.term == other.term\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self.term > other.term\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.term + \": \" + repr(self.posting_list)\n",
    "\n",
    "#example\n",
    "x = Term(\"cat\", 3)\n",
    "y = Term(\"cat\", 6)\n",
    "x.merge(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'description'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-9b7660da800f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m#example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"e.g., this is a text\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-9b7660da800f>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(movie)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmovie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'description'"
     ]
    }
   ],
   "source": [
    "def normalize(text):\n",
    "    '''\n",
    "    remove punctuation and everything that is not a word\n",
    "    '''\n",
    "    #way of matching the text and substitute it\n",
    "    # \\w means not sth alphanumeric\n",
    "    # \\s not a space\n",
    "    # ~ not a dash\n",
    "    no_punctuation = re.sub(r'[\\w^\\s^~]', '', text)\n",
    "    downcase = no_punctuation.lower()\n",
    "    return downcase\n",
    "\n",
    "def tokenize(movie):\n",
    "    text = normalize(movie.description)\n",
    "    return list(text.split())\n",
    "\n",
    "class InvertedIndex:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._dictionary = []\n",
    "    \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        #keys are the tokens\n",
    "        #actual terms are the values\n",
    "        intermediate_dict = {}\n",
    "        for docID, document in enumerate(corpus):\n",
    "            tokens = tokenize(document)\n",
    "            for token in tokens:\n",
    "                term = Term(term, docID)\n",
    "                try:\n",
    "                    #merge the posting lists\n",
    "                    intermediate_dict(token).merge(term)\n",
    "                except KeyError:\n",
    "                    #insert the new id\n",
    "                    intermediate_dict[token] = term\n",
    "            if(docID % 1000 == 0):\n",
    "                print(\"ID: \" + str(docID))\n",
    "        #now we have all the terms and we can create the actual inverted index\n",
    "        idx = cls()\n",
    "        #the dictionary is the sorted list of all the terms\n",
    "        idx._ditionary = sorted(intermediate_dict.values())\n",
    "        return idx\n",
    "    \n",
    "    #we have to index the inverted index using terms\n",
    "    def __getitem__(self, key):\n",
    "        #since everything is sorted in principle we could do a binary search\n",
    "        for term in self._dictionary:\n",
    "            if term.term == key: #the one we are searching for\n",
    "                return term.posting_list\n",
    "            \n",
    "        raise KeyError\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"A dictionary with \" + str(len(self._dictionary)) + \" terms.\"\n",
    "    \n",
    "\n",
    "#example\n",
    "tokenize(normalize(\"e.g., this is a text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Reading the corpus\n",
    "\n",
    "Dependent on the specific corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDescription:\n",
    "    '''\n",
    "    container for al the info we have about the movie\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, title, description):\n",
    "        self.title = title\n",
    "        self.description = description\n",
    "        \n",
    "    def __repr__(self): \n",
    "        return self.title\n",
    "    \n",
    "    \n",
    "def read_movie_description():\n",
    "    #hardcode the names of the dataset's files\n",
    "    filename = 'Lab/plot_summaries.txt'\n",
    "    movie_names = 'Lab/movie.metadata.tsv'\n",
    "    with open(movie_names, 'r') as csv_file:\n",
    "        movie_names = csv.reader(csv_file, delimiter='\\t')\n",
    "        #dictionary where the index will be the ID of the movie\n",
    "        #and the value the title of the movie\n",
    "        names_table = {}\n",
    "        for name in movie_names:\n",
    "            names_table[name[0]] = name[2]\n",
    "    #open the file containing the descriptions\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        description = csv.reader(csv_file, delimiter='\\t')\n",
    "        #corpus is a list of objects\n",
    "        corpus = []\n",
    "        #we wrap into a try block since there are some descriptions\n",
    "        #with errors in the ID\n",
    "        for desc in description:\n",
    "            try: \n",
    "                movie = MovieDescription(names_table[desc[0]], desc[1])\n",
    "                corpus.append(movie)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_movie_description()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = InvertedIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx)\n",
    "idx['batman'] #docId of all the movies containing batman in the description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRsystem:\n",
    "    #in the system we have both the index and the corpus\n",
    "    def __init__(self, corpus, index):\n",
    "        self._corpus = corpus\n",
    "        self._index = index\n",
    "    \n",
    "    #we want to geenrate the entire inverted index \n",
    "    #calling the constructor\n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        index = InvertedIndex.from_corpus(corpus)\n",
    "        return cls(corpus, index)\n",
    "    \n",
    "    def answer_query(self, words): \n",
    "        #same normalization in the query and in the index\n",
    "        norm_words = map(normalize, words)\n",
    "        postings = map(lambda w : self._index[w], norm_words)\n",
    "        plist = reduce(lambda x, y: x.intersection(y), postings)\n",
    "        return plist.get_from_corpus(self._corpus)\n",
    "\n",
    "#permorm a query on a IR system\n",
    "def query(ir, text):\n",
    "    words = text.split()\n",
    "    answer = ir.answer_query(words)\n",
    "    for movie in answer:\n",
    "        print(movie)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = IRsystem(corpus, idx)\n",
    "\n",
    "query(ir, \"frodo Gandalf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
